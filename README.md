# Multivariate Data Analysis of Spotify Music Dataset

This project applies dimensionality reduction, clustering, and classification techniques to explore relationships between musical features and genres in a curated Spotify dataset.

---

## ğŸ“„ Overview

We analyzed **1,686 high-popularity tracks** with 29 variables, including:

- **Audio features:** energy, tempo, danceability, valence, etc.
- **Metadata:** genre, artist

The objective was to uncover latent patterns and genre structures using:

- Principal Component Analysis (PCA)
- Multidimensional Scaling (MDS)
- Multiple Correspondence Analysis (MCA)
- Clustering (Hierarchical and K-Means)
- Discriminant Analysis (QDA) and MANOVA

---

## ğŸ—‚ï¸ Dataset

- **Source:** Kaggle Spotify Music Dataset
- **Preprocessing:**
  - Removed irrelevant metadata (IDs, URLs)
  - Selected the 5 most frequent genres and their top subgenres
  - Standardized numerical variables

---

## ğŸ§ª Methodology

**1ï¸âƒ£ Principal Component Analysis (PCA):**
- Reduced dimensionality to 4 principal components explaining most variance.
- Interpreted axes such as:
  - Energy & Loudness vs. Acousticness
  - Danceability & Brevity
  - Musical Complexity
  - Lyrical Content

**2ï¸âƒ£ Multidimensional Scaling (MDS):**
- Applied metric MDS (Euclidean and Gower distances) to project songs into 2D.
- Gower distance offered effective genre separation.
- Dimensions reflected:
  - Expressive Rhythmic Intensity
  - Lyrical Density vs. Mainstream Appeal

**3ï¸âƒ£ Multiple Correspondence Analysis (MCA):**
- Transformed numeric features into categories.
- Revealed contrasts between:
  - High Danceability/Speechiness
  - High Instrumentalness/Acousticness
  - Energy/Valence gradients

**4ï¸âƒ£ Clustering:**
- Wardâ€™s hierarchical clustering identified 4 interpretable clusters:
  - Calm & Acoustic
  - Popular & Powerful
  - Rhythmic & Spoken
  - Instrumental & Emotional
- K-Means and HCPC yielded overlapping, less interpretable clusters.

**5ï¸âƒ£ Discriminant Analysis and MANOVA:**
- QDA achieved ~70% accuracy in genre classification.
- Speechiness, Acousticness, and Duration were most discriminative.
- Naive Bayes performed worse, mainly confusing Pop and Electronic.

---

## ğŸ” Key Results

| Model               | Accuracy |
|---------------------|----------|
| QDA                 | 70.38%   |
| Naive Bayes         | ~62%     |
| Stepwise Classifier | ~63.5%   |

**Latent Dimensions Identified:**
- Sonic Intensity vs. Acousticness
- Danceability vs. Structural Complexity
- Speechiness vs. Mainstream Popularity

---

## ğŸ“‚ Repository Structure

```
/Spotify_Multivariate_Analysis.zip   â†’ Compressed archive containing:
  â”œâ”€â”€ /high_popularity_spotify_data                       â†’ Preprocessed datasets
  â”œâ”€â”€ /scripts                      â†’ R scripts for all analyses
  â””â”€â”€ /report.pdf                   â†’ PDF report with detailed methodology and results
```

---

## ğŸ’¡ Conclusions

- Dimensionality reduction consistently revealed interpretable axes of musical variation.
- Gower-based MDS improved genre separability compared to Euclidean.
- QDA effectively classified tracks, confirming the predictive value of speechiness and acousticness.
- Clustering exposed latent groupings aligned with musical characteristics.

---

## ğŸ“š References

- Kaggle Spotify Music Dataset
- Acar, N. (2025). Multivariate Data Analysis Lectures. Universitat PolitÃ¨cnica de Catalunya.
- R Documentation

---
